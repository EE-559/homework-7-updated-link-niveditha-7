{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating total UPDRS with M=5,10,15,. each M is evaluated in separate iteration blocks with their respective result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import r2_score\n",
    "from metric_learn import MLKR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f:\\AssignmentsUSC\\Homework7\\homework-7-updated-link-niveditha-7\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "path = os.getcwd()\n",
    "print(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = path+'\\parkinsons_updrs.data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject#</th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>test_time</th>\n",
       "      <th>motor_UPDRS</th>\n",
       "      <th>total_UPDRS</th>\n",
       "      <th>Jitter(%)</th>\n",
       "      <th>Jitter(Abs)</th>\n",
       "      <th>Jitter:RAP</th>\n",
       "      <th>Jitter:PPQ5</th>\n",
       "      <th>...</th>\n",
       "      <th>Shimmer(dB)</th>\n",
       "      <th>Shimmer:APQ3</th>\n",
       "      <th>Shimmer:APQ5</th>\n",
       "      <th>Shimmer:APQ11</th>\n",
       "      <th>Shimmer:DDA</th>\n",
       "      <th>NHR</th>\n",
       "      <th>HNR</th>\n",
       "      <th>RPDE</th>\n",
       "      <th>DFA</th>\n",
       "      <th>PPE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>72</td>\n",
       "      <td>0</td>\n",
       "      <td>5.6431</td>\n",
       "      <td>28.199</td>\n",
       "      <td>34.398</td>\n",
       "      <td>0.00662</td>\n",
       "      <td>0.000034</td>\n",
       "      <td>0.00401</td>\n",
       "      <td>0.00317</td>\n",
       "      <td>...</td>\n",
       "      <td>0.230</td>\n",
       "      <td>0.01438</td>\n",
       "      <td>0.01309</td>\n",
       "      <td>0.01662</td>\n",
       "      <td>0.04314</td>\n",
       "      <td>0.014290</td>\n",
       "      <td>21.640</td>\n",
       "      <td>0.41888</td>\n",
       "      <td>0.54842</td>\n",
       "      <td>0.16006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>72</td>\n",
       "      <td>0</td>\n",
       "      <td>12.6660</td>\n",
       "      <td>28.447</td>\n",
       "      <td>34.894</td>\n",
       "      <td>0.00300</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.00132</td>\n",
       "      <td>0.00150</td>\n",
       "      <td>...</td>\n",
       "      <td>0.179</td>\n",
       "      <td>0.00994</td>\n",
       "      <td>0.01072</td>\n",
       "      <td>0.01689</td>\n",
       "      <td>0.02982</td>\n",
       "      <td>0.011112</td>\n",
       "      <td>27.183</td>\n",
       "      <td>0.43493</td>\n",
       "      <td>0.56477</td>\n",
       "      <td>0.10810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>72</td>\n",
       "      <td>0</td>\n",
       "      <td>19.6810</td>\n",
       "      <td>28.695</td>\n",
       "      <td>35.389</td>\n",
       "      <td>0.00481</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>0.00205</td>\n",
       "      <td>0.00208</td>\n",
       "      <td>...</td>\n",
       "      <td>0.181</td>\n",
       "      <td>0.00734</td>\n",
       "      <td>0.00844</td>\n",
       "      <td>0.01458</td>\n",
       "      <td>0.02202</td>\n",
       "      <td>0.020220</td>\n",
       "      <td>23.047</td>\n",
       "      <td>0.46222</td>\n",
       "      <td>0.54405</td>\n",
       "      <td>0.21014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>72</td>\n",
       "      <td>0</td>\n",
       "      <td>25.6470</td>\n",
       "      <td>28.905</td>\n",
       "      <td>35.810</td>\n",
       "      <td>0.00528</td>\n",
       "      <td>0.000027</td>\n",
       "      <td>0.00191</td>\n",
       "      <td>0.00264</td>\n",
       "      <td>...</td>\n",
       "      <td>0.327</td>\n",
       "      <td>0.01106</td>\n",
       "      <td>0.01265</td>\n",
       "      <td>0.01963</td>\n",
       "      <td>0.03317</td>\n",
       "      <td>0.027837</td>\n",
       "      <td>24.445</td>\n",
       "      <td>0.48730</td>\n",
       "      <td>0.57794</td>\n",
       "      <td>0.33277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>72</td>\n",
       "      <td>0</td>\n",
       "      <td>33.6420</td>\n",
       "      <td>29.187</td>\n",
       "      <td>36.375</td>\n",
       "      <td>0.00335</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>0.00093</td>\n",
       "      <td>0.00130</td>\n",
       "      <td>...</td>\n",
       "      <td>0.176</td>\n",
       "      <td>0.00679</td>\n",
       "      <td>0.00929</td>\n",
       "      <td>0.01819</td>\n",
       "      <td>0.02036</td>\n",
       "      <td>0.011625</td>\n",
       "      <td>26.126</td>\n",
       "      <td>0.47188</td>\n",
       "      <td>0.56122</td>\n",
       "      <td>0.19361</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   subject#  age  sex  test_time  motor_UPDRS  total_UPDRS  Jitter(%)  \\\n",
       "0         1   72    0     5.6431       28.199       34.398    0.00662   \n",
       "1         1   72    0    12.6660       28.447       34.894    0.00300   \n",
       "2         1   72    0    19.6810       28.695       35.389    0.00481   \n",
       "3         1   72    0    25.6470       28.905       35.810    0.00528   \n",
       "4         1   72    0    33.6420       29.187       36.375    0.00335   \n",
       "\n",
       "   Jitter(Abs)  Jitter:RAP  Jitter:PPQ5  ...  Shimmer(dB)  Shimmer:APQ3  \\\n",
       "0     0.000034     0.00401      0.00317  ...        0.230       0.01438   \n",
       "1     0.000017     0.00132      0.00150  ...        0.179       0.00994   \n",
       "2     0.000025     0.00205      0.00208  ...        0.181       0.00734   \n",
       "3     0.000027     0.00191      0.00264  ...        0.327       0.01106   \n",
       "4     0.000020     0.00093      0.00130  ...        0.176       0.00679   \n",
       "\n",
       "   Shimmer:APQ5  Shimmer:APQ11  Shimmer:DDA       NHR     HNR     RPDE  \\\n",
       "0       0.01309        0.01662      0.04314  0.014290  21.640  0.41888   \n",
       "1       0.01072        0.01689      0.02982  0.011112  27.183  0.43493   \n",
       "2       0.00844        0.01458      0.02202  0.020220  23.047  0.46222   \n",
       "3       0.01265        0.01963      0.03317  0.027837  24.445  0.48730   \n",
       "4       0.00929        0.01819      0.02036  0.011625  26.126  0.47188   \n",
       "\n",
       "       DFA      PPE  \n",
       "0  0.54842  0.16006  \n",
       "1  0.56477  0.10810  \n",
       "2  0.54405  0.21014  \n",
       "3  0.57794  0.33277  \n",
       "4  0.56122  0.19361  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Display the first few rows of the dataset\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data split into training and testing sets.\n",
      "Data standardized.\n",
      "X_train_scaled shape: (4112, 19)\n",
      "X_test_scaled shape: (1763, 19)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from metric_learn import MLKR\n",
    "\n",
    "\n",
    "X = df.drop(columns=['motor_UPDRS', 'total_UPDRS', 'subject#'])\n",
    "y_total = df['total_UPDRS']\n",
    "\n",
    "# Split the data into training (70%) and testing (30%) sets\n",
    "X_train, X_test, y_total_train, y_total_test = train_test_split(\n",
    "    X, y_total, test_size=0.3, random_state=42\n",
    ")\n",
    "print(\"Data split into training and testing sets.\")\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(\"Data standardized.\")\n",
    "print(f\"X_train_scaled shape: {X_train_scaled.shape}\")\n",
    "print(f\"X_test_scaled shape: {X_test_scaled.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_with_mlkr(X_train, y_train, X_test, n_components):\n",
    "    print(f\"Applying MLKR with {n_components} components...\")\n",
    "    mlkr = MLKR(init='auto', n_components=n_components)\n",
    "    mlkr.fit(X_train, y_train)\n",
    "    X_train_mlkr = mlkr.transform(X_train)\n",
    "    X_test_mlkr = mlkr.transform(X_test)\n",
    "    print(f\"MLKR transformation done. X_train_mlkr shape: {X_train_mlkr.shape}, X_test_mlkr shape: {X_test_mlkr.shape}\")\n",
    "    return X_train_mlkr, X_test_mlkr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "\n",
    "def tune_krr_hyperparameters(X_train, y_train):\n",
    "    print(\"Starting hyperparameter tuning...\")\n",
    "    krr = KernelRidge(kernel='rbf')\n",
    "    param_grid = {\n",
    "        'alpha': [0.1, 1, 10],\n",
    "        'gamma': [0.001, 0.01, 0.1, 1]\n",
    "    }\n",
    "    grid_search = GridSearchCV(krr, param_grid, cv=5, scoring='r2', verbose=2, n_jobs=-1)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    print(\"Hyperparameter tuning completed.\")\n",
    "    print(\"Best parameters found: \", grid_search.best_params_)\n",
    "    print(\"Best R^2 score: \", grid_search.best_score_)\n",
    "    \n",
    "    return grid_search.best_estimator_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating number of components: 5 for Total UPDRS\n",
      "\n",
      "Fold 1\n",
      "Training set size: 3289, Validation set size: 823\n",
      "Applying MLKR with 5 components...\n",
      "MLKR transformation done. X_train_mlkr shape: (3289, 5), X_test_mlkr shape: (823, 5)\n",
      "Starting hyperparameter tuning...\n",
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n",
      "Hyperparameter tuning completed.\n",
      "Best parameters found:  {'alpha': 0.1, 'gamma': 0.001}\n",
      "Best R^2 score:  0.8289713781067973\n",
      "Training the model...\n",
      "Model training and prediction completed.\n",
      "R^2 Train: 0.8682788985275673, R^2 Test: 0.8386162517536062\n",
      "\n",
      "Fold 2\n",
      "Training set size: 3289, Validation set size: 823\n",
      "Applying MLKR with 5 components...\n",
      "MLKR transformation done. X_train_mlkr shape: (3289, 5), X_test_mlkr shape: (823, 5)\n",
      "Starting hyperparameter tuning...\n",
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n",
      "Hyperparameter tuning completed.\n",
      "Best parameters found:  {'alpha': 0.1, 'gamma': 0.01}\n",
      "Best R^2 score:  0.8583751044551982\n",
      "Training the model...\n",
      "Model training and prediction completed.\n",
      "R^2 Train: 0.9664941842364294, R^2 Test: 0.8746260096327008\n",
      "\n",
      "Fold 3\n",
      "Training set size: 3290, Validation set size: 822\n",
      "Applying MLKR with 5 components...\n",
      "MLKR transformation done. X_train_mlkr shape: (3290, 5), X_test_mlkr shape: (822, 5)\n",
      "Starting hyperparameter tuning...\n",
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n",
      "Hyperparameter tuning completed.\n",
      "Best parameters found:  {'alpha': 0.1, 'gamma': 0.001}\n",
      "Best R^2 score:  0.7936495028679061\n",
      "Training the model...\n",
      "Model training and prediction completed.\n",
      "R^2 Train: 0.8334997157529961, R^2 Test: 0.7847025354600128\n",
      "\n",
      "Fold 4\n",
      "Training set size: 3290, Validation set size: 822\n",
      "Applying MLKR with 5 components...\n",
      "MLKR transformation done. X_train_mlkr shape: (3290, 5), X_test_mlkr shape: (822, 5)\n",
      "Starting hyperparameter tuning...\n",
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n",
      "Hyperparameter tuning completed.\n",
      "Best parameters found:  {'alpha': 0.1, 'gamma': 0.01}\n",
      "Best R^2 score:  0.8000825848370763\n",
      "Training the model...\n",
      "Model training and prediction completed.\n",
      "R^2 Train: 0.9804776067674424, R^2 Test: 0.8092555257445987\n",
      "\n",
      "Fold 5\n",
      "Training set size: 3290, Validation set size: 822\n",
      "Applying MLKR with 5 components...\n",
      "MLKR transformation done. X_train_mlkr shape: (3290, 5), X_test_mlkr shape: (822, 5)\n",
      "Starting hyperparameter tuning...\n",
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n",
      "Hyperparameter tuning completed.\n",
      "Best parameters found:  {'alpha': 0.1, 'gamma': 0.01}\n",
      "Best R^2 score:  0.8738363763587144\n",
      "Training the model...\n",
      "Model training and prediction completed.\n",
      "R^2 Train: 0.9641501290467901, R^2 Test: 0.8583525335532287\n",
      "Total UPDRS Results:\n",
      "   R2_test_mean  R2_train_mean\n",
      "5      0.833111        0.92258\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "results_total = {}\n",
    "\n",
    "# Evaluate for different values of M\n",
    "for M in [5]:\n",
    "    print(f\"\\nEvaluating number of components: {M} for Total UPDRS\")\n",
    "    r2_train_scores_total = []\n",
    "    r2_test_scores_total = []\n",
    "\n",
    "    for fold, (train_index, val_index) in enumerate(kf.split(X_train_scaled)):\n",
    "        print(f\"\\nFold {fold + 1}\")\n",
    "        X_train_fold, X_val_fold = X_train_scaled[train_index], X_train_scaled[val_index]\n",
    "        y_train_fold_total, y_val_fold_total = y_total_train.iloc[train_index], y_total_train.iloc[val_index]\n",
    "        \n",
    "        print(f\"Training set size: {X_train_fold.shape[0]}, Validation set size: {X_val_fold.shape[0]}\")\n",
    "\n",
    "        # Transform with MLKR\n",
    "        X_train_fold_mlkr, X_val_fold_mlkr = transform_with_mlkr(X_train_fold, y_train_fold_total, X_val_fold, n_components=M)\n",
    "\n",
    "        # Perform hyperparameter tuning\n",
    "        best_krr = tune_krr_hyperparameters(X_train_fold_mlkr, y_train_fold_total)\n",
    "\n",
    "        # Train and evaluate the model\n",
    "        print(\"Training the model...\")\n",
    "        best_krr.fit(X_train_fold_mlkr, y_train_fold_total)\n",
    "        y_train_pred_total = best_krr.predict(X_train_fold_mlkr)\n",
    "        y_val_pred_total = best_krr.predict(X_val_fold_mlkr)\n",
    "        print(\"Model training and prediction completed.\")\n",
    "\n",
    "        r2_train_total = r2_score(y_train_fold_total, y_train_pred_total)\n",
    "        r2_test_total = r2_score(y_val_fold_total, y_val_pred_total)\n",
    "        print(f\"R^2 Train: {r2_train_total}, R^2 Test: {r2_test_total}\")\n",
    "\n",
    "        r2_train_scores_total.append(r2_train_total)\n",
    "        r2_test_scores_total.append(r2_test_total)\n",
    "\n",
    "    results_total[M] = {\n",
    "        'R2_train_mean': np.mean(r2_train_scores_total),\n",
    "        'R2_test_mean': np.mean(r2_test_scores_total)\n",
    "    }\n",
    "\n",
    "print(\"Total UPDRS Results:\")\n",
    "total_results_df = pd.DataFrame(results_total).T\n",
    "print(total_results_df)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the above best hyper parameter for next iterations to speed up the process: (code remains same)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "\n",
    "def tune_krr_hyperparameters(X_train, y_train):\n",
    "    print(\"Starting hyperparameter tuning...\")\n",
    "    krr = KernelRidge(kernel='rbf')\n",
    "    param_grid = {\n",
    "        'alpha': [0.1, 1, 10],\n",
    "        'gamma': [0.001, 0.01, 0.1, 1]\n",
    "    }\n",
    "    grid_search = GridSearchCV(krr, param_grid, cv=5, scoring='r2', verbose=2, n_jobs=-1)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    print(\"Hyperparameter tuning completed.\")\n",
    "    print(\"Best parameters found: \", grid_search.best_params_)\n",
    "    print(\"Best R^2 score: \", grid_search.best_score_)\n",
    "    \n",
    "    return grid_search.best_estimator_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating number of components: 10 for Total UPDRS\n",
      "\n",
      "Fold 1\n",
      "Training set size: 3289, Validation set size: 823\n",
      "Applying MLKR with 10 components...\n",
      "MLKR transformation done. X_train_mlkr shape: (3289, 10), X_test_mlkr shape: (823, 10)\n",
      "Starting hyperparameter tuning...\n",
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n",
      "Hyperparameter tuning completed.\n",
      "Best parameters found:  {'alpha': 0.1, 'gamma': 0.01}\n",
      "Best R^2 score:  0.7712998707466774\n",
      "Training the model...\n",
      "Model training and prediction completed.\n",
      "R^2 Train: 0.9829972083096832, R^2 Test: 0.7972693518402463\n",
      "\n",
      "Fold 2\n",
      "Training set size: 3289, Validation set size: 823\n",
      "Applying MLKR with 10 components...\n",
      "MLKR transformation done. X_train_mlkr shape: (3289, 10), X_test_mlkr shape: (823, 10)\n",
      "Starting hyperparameter tuning...\n",
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n",
      "Hyperparameter tuning completed.\n",
      "Best parameters found:  {'alpha': 0.1, 'gamma': 0.001}\n",
      "Best R^2 score:  0.7357890728447769\n",
      "Training the model...\n",
      "Model training and prediction completed.\n",
      "R^2 Train: 0.8071331222652678, R^2 Test: 0.7742831429456805\n",
      "\n",
      "Fold 3\n",
      "Training set size: 3290, Validation set size: 822\n",
      "Applying MLKR with 10 components...\n",
      "MLKR transformation done. X_train_mlkr shape: (3290, 10), X_test_mlkr shape: (822, 10)\n",
      "Starting hyperparameter tuning...\n",
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n",
      "Hyperparameter tuning completed.\n",
      "Best parameters found:  {'alpha': 0.1, 'gamma': 0.01}\n",
      "Best R^2 score:  0.7739081867989943\n",
      "Training the model...\n",
      "Model training and prediction completed.\n",
      "R^2 Train: 0.9787496029873015, R^2 Test: 0.8016159352833561\n",
      "\n",
      "Fold 4\n",
      "Training set size: 3290, Validation set size: 822\n",
      "Applying MLKR with 10 components...\n",
      "MLKR transformation done. X_train_mlkr shape: (3290, 10), X_test_mlkr shape: (822, 10)\n",
      "Starting hyperparameter tuning...\n",
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n",
      "Hyperparameter tuning completed.\n",
      "Best parameters found:  {'alpha': 0.1, 'gamma': 0.01}\n",
      "Best R^2 score:  0.7667965885991296\n",
      "Training the model...\n",
      "Model training and prediction completed.\n",
      "R^2 Train: 0.9635884707799497, R^2 Test: 0.7863724325166355\n",
      "\n",
      "Fold 5\n",
      "Training set size: 3290, Validation set size: 822\n",
      "Applying MLKR with 10 components...\n",
      "MLKR transformation done. X_train_mlkr shape: (3290, 10), X_test_mlkr shape: (822, 10)\n",
      "Starting hyperparameter tuning...\n",
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n",
      "Hyperparameter tuning completed.\n",
      "Best parameters found:  {'alpha': 0.1, 'gamma': 0.01}\n",
      "Best R^2 score:  0.781775564122898\n",
      "Training the model...\n",
      "Model training and prediction completed.\n",
      "R^2 Train: 0.9700050799192848, R^2 Test: 0.8232821033381795\n",
      "Total UPDRS Results:\n",
      "    R2_test_mean  R2_train_mean\n",
      "10      0.796565       0.940495\n"
     ]
    }
   ],
   "source": [
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "results_total = {}\n",
    "\n",
    "# Evaluate for different values of M\n",
    "for M in [10]:\n",
    "    print(f\"\\nEvaluating number of components: {M} for Total UPDRS\")\n",
    "    r2_train_scores_total = []\n",
    "    r2_test_scores_total = []\n",
    "\n",
    "    for fold, (train_index, val_index) in enumerate(kf.split(X_train_scaled)):\n",
    "        print(f\"\\nFold {fold + 1}\")\n",
    "        X_train_fold, X_val_fold = X_train_scaled[train_index], X_train_scaled[val_index]\n",
    "        y_train_fold_total, y_val_fold_total = y_total_train.iloc[train_index], y_total_train.iloc[val_index]\n",
    "        \n",
    "        print(f\"Training set size: {X_train_fold.shape[0]}, Validation set size: {X_val_fold.shape[0]}\")\n",
    "\n",
    "        # Transform with MLKR\n",
    "        X_train_fold_mlkr, X_val_fold_mlkr = transform_with_mlkr(X_train_fold, y_train_fold_total, X_val_fold, n_components=M)\n",
    "\n",
    "        # Perform hyperparameter tuning\n",
    "        best_krr = tune_krr_hyperparameters(X_train_fold_mlkr, y_train_fold_total)\n",
    "\n",
    "        # Train and evaluate the model\n",
    "        print(\"Training the model...\")\n",
    "        best_krr.fit(X_train_fold_mlkr, y_train_fold_total)\n",
    "        y_train_pred_total = best_krr.predict(X_train_fold_mlkr)\n",
    "        y_val_pred_total = best_krr.predict(X_val_fold_mlkr)\n",
    "        print(\"Model training and prediction completed.\")\n",
    "\n",
    "        r2_train_total = r2_score(y_train_fold_total, y_train_pred_total)\n",
    "        r2_test_total = r2_score(y_val_fold_total, y_val_pred_total)\n",
    "        print(f\"R^2 Train: {r2_train_total}, R^2 Test: {r2_test_total}\")\n",
    "\n",
    "        r2_train_scores_total.append(r2_train_total)\n",
    "        r2_test_scores_total.append(r2_test_total)\n",
    "\n",
    "    results_total[M] = {\n",
    "        'R2_train_mean': np.mean(r2_train_scores_total),\n",
    "        'R2_test_mean': np.mean(r2_test_scores_total)\n",
    "    }\n",
    "\n",
    "print(\"Total UPDRS Results:\")\n",
    "total_results_df = pd.DataFrame(results_total).T\n",
    "print(total_results_df)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total UPDRS Results:\n",
      "    R2_test_mean  R2_train_mean\n",
      "10      0.796565       0.940495\n"
     ]
    }
   ],
   "source": [
    "print(\"Total UPDRS Results:\")\n",
    "total_results_df = pd.DataFrame(results_total).T\n",
    "print(total_results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating number of components: 15 for Total UPDRS\n",
      "\n",
      "Fold 1\n",
      "Training set size: 3289, Validation set size: 823\n",
      "Applying MLKR with 15 components...\n",
      "MLKR transformation done. X_train_mlkr shape: (3289, 15), X_test_mlkr shape: (823, 15)\n",
      "Starting hyperparameter tuning...\n",
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n",
      "Hyperparameter tuning completed.\n",
      "Best parameters found:  {'alpha': 0.1, 'gamma': 0.01}\n",
      "Best R^2 score:  0.7764440719681757\n",
      "Training the model...\n",
      "Model training and prediction completed.\n",
      "R^2 Train: 0.9802953920230464, R^2 Test: 0.7894839881732673\n",
      "\n",
      "Fold 2\n",
      "Training set size: 3289, Validation set size: 823\n",
      "Applying MLKR with 15 components...\n",
      "MLKR transformation done. X_train_mlkr shape: (3289, 15), X_test_mlkr shape: (823, 15)\n",
      "Starting hyperparameter tuning...\n",
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n",
      "Hyperparameter tuning completed.\n",
      "Best parameters found:  {'alpha': 0.1, 'gamma': 0.001}\n",
      "Best R^2 score:  0.7501330764451435\n",
      "Training the model...\n",
      "Model training and prediction completed.\n",
      "R^2 Train: 0.8254134017051858, R^2 Test: 0.7863164226477212\n",
      "\n",
      "Fold 3\n",
      "Training set size: 3290, Validation set size: 822\n",
      "Applying MLKR with 15 components...\n",
      "MLKR transformation done. X_train_mlkr shape: (3290, 15), X_test_mlkr shape: (822, 15)\n",
      "Starting hyperparameter tuning...\n",
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n",
      "Hyperparameter tuning completed.\n",
      "Best parameters found:  {'alpha': 0.1, 'gamma': 0.001}\n",
      "Best R^2 score:  0.7345313803212367\n",
      "Training the model...\n",
      "Model training and prediction completed.\n",
      "R^2 Train: 0.7921349595438347, R^2 Test: 0.728857001481003\n",
      "\n",
      "Fold 4\n",
      "Training set size: 3290, Validation set size: 822\n",
      "Applying MLKR with 15 components...\n",
      "MLKR transformation done. X_train_mlkr shape: (3290, 15), X_test_mlkr shape: (822, 15)\n",
      "Starting hyperparameter tuning...\n",
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n",
      "Hyperparameter tuning completed.\n",
      "Best parameters found:  {'alpha': 0.1, 'gamma': 0.01}\n",
      "Best R^2 score:  0.7390265635509004\n",
      "Training the model...\n",
      "Model training and prediction completed.\n",
      "R^2 Train: 0.9777975108155552, R^2 Test: 0.7539059203247496\n",
      "\n",
      "Fold 5\n",
      "Training set size: 3290, Validation set size: 822\n",
      "Applying MLKR with 15 components...\n",
      "MLKR transformation done. X_train_mlkr shape: (3290, 15), X_test_mlkr shape: (822, 15)\n",
      "Starting hyperparameter tuning...\n",
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n",
      "Hyperparameter tuning completed.\n",
      "Best parameters found:  {'alpha': 0.1, 'gamma': 0.01}\n",
      "Best R^2 score:  0.7865320082810636\n",
      "Training the model...\n",
      "Model training and prediction completed.\n",
      "R^2 Train: 0.9653246094041393, R^2 Test: 0.8126735874831509\n",
      "Total UPDRS Results:\n",
      "    R2_test_mean  R2_train_mean\n",
      "15      0.774247       0.908193\n"
     ]
    }
   ],
   "source": [
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "results_total_15= {}\n",
    "\n",
    "# Evaluate for different values of M\n",
    "for M in [15]:\n",
    "    print(f\"\\nEvaluating number of components: {M} for Total UPDRS\")\n",
    "    r2_train_scores_total = []\n",
    "    r2_test_scores_total = []\n",
    "\n",
    "    for fold, (train_index, val_index) in enumerate(kf.split(X_train_scaled)):\n",
    "        print(f\"\\nFold {fold + 1}\")\n",
    "        X_train_fold, X_val_fold = X_train_scaled[train_index], X_train_scaled[val_index]\n",
    "        y_train_fold_total, y_val_fold_total = y_total_train.iloc[train_index], y_total_train.iloc[val_index]\n",
    "        \n",
    "        print(f\"Training set size: {X_train_fold.shape[0]}, Validation set size: {X_val_fold.shape[0]}\")\n",
    "\n",
    "        # Transform with MLKR\n",
    "        X_train_fold_mlkr, X_val_fold_mlkr = transform_with_mlkr(X_train_fold, y_train_fold_total, X_val_fold, n_components=M)\n",
    "\n",
    "        # Perform hyperparameter tuning\n",
    "        best_krr = tune_krr_hyperparameters(X_train_fold_mlkr, y_train_fold_total)\n",
    "\n",
    "        # Train and evaluate the model\n",
    "        print(\"Training the model...\")\n",
    "        best_krr.fit(X_train_fold_mlkr, y_train_fold_total)\n",
    "        y_train_pred_total = best_krr.predict(X_train_fold_mlkr)\n",
    "        y_val_pred_total = best_krr.predict(X_val_fold_mlkr)\n",
    "        print(\"Model training and prediction completed.\")\n",
    "\n",
    "        r2_train_total = r2_score(y_train_fold_total, y_train_pred_total)\n",
    "        r2_test_total = r2_score(y_val_fold_total, y_val_pred_total)\n",
    "        print(f\"R^2 Train: {r2_train_total}, R^2 Test: {r2_test_total}\")\n",
    "\n",
    "        r2_train_scores_total.append(r2_train_total)\n",
    "        r2_test_scores_total.append(r2_test_total)\n",
    "\n",
    "    results_total_15[M] = {\n",
    "        'R2_train_mean': np.mean(r2_train_scores_total),\n",
    "        'R2_test_mean': np.mean(r2_test_scores_total)\n",
    "    }\n",
    "\n",
    "print(\"Total UPDRS Results:\")\n",
    "total_results_df_15 = pd.DataFrame(results_total_15).T\n",
    "print(total_results_df_15)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total UPDRS Results:\n",
      "    R2_test_mean  R2_train_mean\n",
      "15      0.774247       0.908193\n"
     ]
    }
   ],
   "source": [
    "print(\"Total UPDRS Results:\")\n",
    "total_results_df_15 = pd.DataFrame(results_total_15).T\n",
    "print(total_results_df_15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For M=p and total UPDRS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating number of components: 19 for Total UPDRS\n",
      "\n",
      "Fold 1\n",
      "Training set size: 3289, Validation set size: 823\n",
      "Applying MLKR with 19 components...\n",
      "MLKR transformation done. X_train_mlkr shape: (3289, 19), X_test_mlkr shape: (823, 19)\n",
      "Starting hyperparameter tuning...\n",
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n",
      "Hyperparameter tuning completed.\n",
      "Best parameters found:  {'alpha': 0.1, 'gamma': 0.001}\n",
      "Best R^2 score:  0.7467854241980765\n",
      "Training the model...\n",
      "Model training and prediction completed.\n",
      "R^2 Train: 0.807813919831189, R^2 Test: 0.7677421299852951\n",
      "\n",
      "Fold 2\n",
      "Training set size: 3289, Validation set size: 823\n",
      "Applying MLKR with 19 components...\n",
      "MLKR transformation done. X_train_mlkr shape: (3289, 19), X_test_mlkr shape: (823, 19)\n",
      "Starting hyperparameter tuning...\n",
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n",
      "Hyperparameter tuning completed.\n",
      "Best parameters found:  {'alpha': 0.1, 'gamma': 0.001}\n",
      "Best R^2 score:  0.779050979526171\n",
      "Training the model...\n",
      "Model training and prediction completed.\n",
      "R^2 Train: 0.8568396783530042, R^2 Test: 0.8094068419573368\n",
      "\n",
      "Fold 3\n",
      "Training set size: 3290, Validation set size: 822\n",
      "Applying MLKR with 19 components...\n",
      "MLKR transformation done. X_train_mlkr shape: (3290, 19), X_test_mlkr shape: (822, 19)\n",
      "Starting hyperparameter tuning...\n",
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n",
      "Hyperparameter tuning completed.\n",
      "Best parameters found:  {'alpha': 0.1, 'gamma': 0.001}\n",
      "Best R^2 score:  0.7533830785641806\n",
      "Training the model...\n",
      "Model training and prediction completed.\n",
      "R^2 Train: 0.8197000395730938, R^2 Test: 0.7604050743151374\n",
      "\n",
      "Fold 4\n",
      "Training set size: 3290, Validation set size: 822\n",
      "Applying MLKR with 19 components...\n",
      "MLKR transformation done. X_train_mlkr shape: (3290, 19), X_test_mlkr shape: (822, 19)\n",
      "Starting hyperparameter tuning...\n",
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n",
      "Hyperparameter tuning completed.\n",
      "Best parameters found:  {'alpha': 0.1, 'gamma': 0.01}\n",
      "Best R^2 score:  0.715870842174887\n",
      "Training the model...\n",
      "Model training and prediction completed.\n",
      "R^2 Train: 0.9795450504699286, R^2 Test: 0.7339737033949999\n",
      "\n",
      "Fold 5\n",
      "Training set size: 3290, Validation set size: 822\n",
      "Applying MLKR with 19 components...\n",
      "MLKR transformation done. X_train_mlkr shape: (3290, 19), X_test_mlkr shape: (822, 19)\n",
      "Starting hyperparameter tuning...\n",
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n",
      "Hyperparameter tuning completed.\n",
      "Best parameters found:  {'alpha': 0.1, 'gamma': 0.01}\n",
      "Best R^2 score:  0.7844237326061263\n",
      "Training the model...\n",
      "Model training and prediction completed.\n",
      "R^2 Train: 0.9754085839002512, R^2 Test: 0.820474804613532\n",
      "Total UPDRS Results:\n",
      "    R2_test_mean  R2_train_mean\n",
      "19      0.778401       0.887861\n"
     ]
    }
   ],
   "source": [
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "results_total_p= {}\n",
    "\n",
    "# Evaluate for different values of M\n",
    "for M in [X_train_scaled.shape[1]]:\n",
    "    print(f\"\\nEvaluating number of components: {M} for Total UPDRS\")\n",
    "    r2_train_scores_total = []\n",
    "    r2_test_scores_total = []\n",
    "\n",
    "    for fold, (train_index, val_index) in enumerate(kf.split(X_train_scaled)):\n",
    "        print(f\"\\nFold {fold + 1}\")\n",
    "        X_train_fold, X_val_fold = X_train_scaled[train_index], X_train_scaled[val_index]\n",
    "        y_train_fold_total, y_val_fold_total = y_total_train.iloc[train_index], y_total_train.iloc[val_index]\n",
    "        \n",
    "        print(f\"Training set size: {X_train_fold.shape[0]}, Validation set size: {X_val_fold.shape[0]}\")\n",
    "\n",
    "        # Transform with MLKR\n",
    "        X_train_fold_mlkr, X_val_fold_mlkr = transform_with_mlkr(X_train_fold, y_train_fold_total, X_val_fold, n_components=M)\n",
    "\n",
    "        # Perform hyperparameter tuning\n",
    "        best_krr = tune_krr_hyperparameters(X_train_fold_mlkr, y_train_fold_total)\n",
    "\n",
    "        # Train and evaluate the model\n",
    "        print(\"Training the model...\")\n",
    "        best_krr.fit(X_train_fold_mlkr, y_train_fold_total)\n",
    "        y_train_pred_total = best_krr.predict(X_train_fold_mlkr)\n",
    "        y_val_pred_total = best_krr.predict(X_val_fold_mlkr)\n",
    "        print(\"Model training and prediction completed.\")\n",
    "\n",
    "        r2_train_total = r2_score(y_train_fold_total, y_train_pred_total)\n",
    "        r2_test_total = r2_score(y_val_fold_total, y_val_pred_total)\n",
    "        print(f\"R^2 Train: {r2_train_total}, R^2 Test: {r2_test_total}\")\n",
    "\n",
    "        r2_train_scores_total.append(r2_train_total)\n",
    "        r2_test_scores_total.append(r2_test_total)\n",
    "\n",
    "    results_total_p[M] = {\n",
    "        'R2_train_mean': np.mean(r2_train_scores_total),\n",
    "        'R2_test_mean': np.mean(r2_test_scores_total)\n",
    "    }\n",
    "\n",
    "print(\"Total UPDRS Results:\")\n",
    "total_results_df_p = pd.DataFrame(results_total_p).T\n",
    "print(total_results_df_p)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NEURAL NETWORKS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use sklearn's neural network implementation to train a neural network with two\n",
    "outputs that predicts motor UPDRS and total UPDRS. Use a single layer. You are\n",
    "responsible to determine other architectural parameters of the network, including\n",
    "the number of neurons in the hidden and output layers, method of optimization,\n",
    "type of activation functions, and the L2 \\regularization\" parameter etc. You\n",
    "should determine the design parameters via trial and error, by testing your trained\n",
    "network on the test set and choosing the architecture that yields the smallest test\n",
    "error. For this part, set early-stopping=False. Remember to standardize your\n",
    "features. Report your R2 on both training and test sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference -https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPRegressor.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data split and standardized.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "X = df.drop(columns=['motor_UPDRS', 'total_UPDRS', 'subject#'])\n",
    "y = df[['motor_UPDRS', 'total_UPDRS']]\n",
    "\n",
    "# Split the data into training (70%) and testing (30%) sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(\"Data split and standardized.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural network trained with hidden_layer_sizes=(100,), activation=relu, solver=adam, alpha=0.0001, learning_rate=constant\n",
      "R^2 Train: 0.7494489138402389, R^2 Test: 0.6799432320439998\n"
     ]
    }
   ],
   "source": [
    "def train_evaluate_nn(X_train, y_train, X_test, y_test, hidden_layer_sizes=(100,), activation='relu', solver='adam', alpha=0.0001, learning_rate='constant'):\n",
    "    model = MLPRegressor(\n",
    "        hidden_layer_sizes=hidden_layer_sizes,\n",
    "        activation=activation,\n",
    "        solver=solver,\n",
    "        alpha=alpha,\n",
    "        learning_rate=learning_rate,\n",
    "        max_iter=2000,\n",
    "        early_stopping=False,\n",
    "        random_state=42\n",
    "    )\n",
    "    model.fit(X_train, y_train)\n",
    "    print(f\"Neural network trained with hidden_layer_sizes={hidden_layer_sizes}, activation={activation}, solver={solver}, alpha={alpha}, learning_rate={learning_rate}\")\n",
    "    \n",
    "    y_train_pred = model.predict(X_train)\n",
    "    y_test_pred = model.predict(X_test)\n",
    "    \n",
    "    r2_train = r2_score(y_train, y_train_pred)\n",
    "    r2_test = r2_score(y_test, y_test_pred)\n",
    "    \n",
    "    print(f\"R^2 Train: {r2_train}, R^2 Test: {r2_test}\")\n",
    "    \n",
    "    return r2_train, r2_test\n",
    "\n",
    "# Example usage\n",
    "r2_train, r2_test = train_evaluate_nn(X_train_scaled, y_train, X_test_scaled, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Trying hidden_layer_sizes=(50,), activation=relu, solver=adam, alpha=0.0001, learning_rate=constant\n",
      "Neural network trained with hidden_layer_sizes=(50,), activation=relu, solver=adam, alpha=0.0001, learning_rate=constant\n",
      "R^2 Train: 0.6811216408248943, R^2 Test: 0.6167900863747688\n",
      "\n",
      "Trying hidden_layer_sizes=(50,), activation=relu, solver=adam, alpha=0.0001, learning_rate=adaptive\n",
      "Neural network trained with hidden_layer_sizes=(50,), activation=relu, solver=adam, alpha=0.0001, learning_rate=adaptive\n",
      "R^2 Train: 0.6811216408248943, R^2 Test: 0.6167900863747688\n",
      "\n",
      "Trying hidden_layer_sizes=(50,), activation=relu, solver=adam, alpha=0.001, learning_rate=constant\n",
      "Neural network trained with hidden_layer_sizes=(50,), activation=relu, solver=adam, alpha=0.001, learning_rate=constant\n",
      "R^2 Train: 0.6820017061811813, R^2 Test: 0.6160481562291011\n",
      "\n",
      "Trying hidden_layer_sizes=(50,), activation=relu, solver=adam, alpha=0.001, learning_rate=adaptive\n",
      "Neural network trained with hidden_layer_sizes=(50,), activation=relu, solver=adam, alpha=0.001, learning_rate=adaptive\n",
      "R^2 Train: 0.6820017061811813, R^2 Test: 0.6160481562291011\n",
      "\n",
      "Trying hidden_layer_sizes=(50,), activation=relu, solver=adam, alpha=0.01, learning_rate=constant\n",
      "Neural network trained with hidden_layer_sizes=(50,), activation=relu, solver=adam, alpha=0.01, learning_rate=constant\n",
      "R^2 Train: 0.6846971064669936, R^2 Test: 0.6189795225573178\n",
      "\n",
      "Trying hidden_layer_sizes=(50,), activation=relu, solver=adam, alpha=0.01, learning_rate=adaptive\n",
      "Neural network trained with hidden_layer_sizes=(50,), activation=relu, solver=adam, alpha=0.01, learning_rate=adaptive\n",
      "R^2 Train: 0.6846971064669936, R^2 Test: 0.6189795225573178\n",
      "\n",
      "Trying hidden_layer_sizes=(50,), activation=relu, solver=sgd, alpha=0.0001, learning_rate=constant\n",
      "Neural network trained with hidden_layer_sizes=(50,), activation=relu, solver=sgd, alpha=0.0001, learning_rate=constant\n",
      "R^2 Train: 0.6295731887731854, R^2 Test: 0.5800604070799841\n",
      "\n",
      "Trying hidden_layer_sizes=(50,), activation=relu, solver=sgd, alpha=0.0001, learning_rate=adaptive\n",
      "Neural network trained with hidden_layer_sizes=(50,), activation=relu, solver=sgd, alpha=0.0001, learning_rate=adaptive\n",
      "R^2 Train: 0.7097576765776729, R^2 Test: 0.6272157199365613\n",
      "\n",
      "Trying hidden_layer_sizes=(50,), activation=relu, solver=sgd, alpha=0.001, learning_rate=constant\n",
      "Neural network trained with hidden_layer_sizes=(50,), activation=relu, solver=sgd, alpha=0.001, learning_rate=constant\n",
      "R^2 Train: 0.6306582284712949, R^2 Test: 0.5688373488521183\n",
      "\n",
      "Trying hidden_layer_sizes=(50,), activation=relu, solver=sgd, alpha=0.001, learning_rate=adaptive\n",
      "Neural network trained with hidden_layer_sizes=(50,), activation=relu, solver=sgd, alpha=0.001, learning_rate=adaptive\n",
      "R^2 Train: 0.7321990965704677, R^2 Test: 0.6301900799048199\n",
      "\n",
      "Trying hidden_layer_sizes=(50,), activation=relu, solver=sgd, alpha=0.01, learning_rate=constant\n",
      "Neural network trained with hidden_layer_sizes=(50,), activation=relu, solver=sgd, alpha=0.01, learning_rate=constant\n",
      "R^2 Train: 0.647008679337254, R^2 Test: 0.5792061447083245\n",
      "\n",
      "Trying hidden_layer_sizes=(50,), activation=relu, solver=sgd, alpha=0.01, learning_rate=adaptive\n",
      "Neural network trained with hidden_layer_sizes=(50,), activation=relu, solver=sgd, alpha=0.01, learning_rate=adaptive\n",
      "R^2 Train: 0.7193744013574355, R^2 Test: 0.6183494867053011\n",
      "\n",
      "Trying hidden_layer_sizes=(50,), activation=tanh, solver=adam, alpha=0.0001, learning_rate=constant\n",
      "Neural network trained with hidden_layer_sizes=(50,), activation=tanh, solver=adam, alpha=0.0001, learning_rate=constant\n",
      "R^2 Train: 0.8075085141112663, R^2 Test: 0.7197764930632168\n",
      "\n",
      "Trying hidden_layer_sizes=(50,), activation=tanh, solver=adam, alpha=0.0001, learning_rate=adaptive\n",
      "Neural network trained with hidden_layer_sizes=(50,), activation=tanh, solver=adam, alpha=0.0001, learning_rate=adaptive\n",
      "R^2 Train: 0.8075085141112663, R^2 Test: 0.7197764930632168\n",
      "\n",
      "Trying hidden_layer_sizes=(50,), activation=tanh, solver=adam, alpha=0.001, learning_rate=constant\n",
      "Neural network trained with hidden_layer_sizes=(50,), activation=tanh, solver=adam, alpha=0.001, learning_rate=constant\n",
      "R^2 Train: 0.8075434061779765, R^2 Test: 0.7197737155608673\n",
      "\n",
      "Trying hidden_layer_sizes=(50,), activation=tanh, solver=adam, alpha=0.001, learning_rate=adaptive\n",
      "Neural network trained with hidden_layer_sizes=(50,), activation=tanh, solver=adam, alpha=0.001, learning_rate=adaptive\n",
      "R^2 Train: 0.8075434061779765, R^2 Test: 0.7197737155608673\n",
      "\n",
      "Trying hidden_layer_sizes=(50,), activation=tanh, solver=adam, alpha=0.01, learning_rate=constant\n",
      "Neural network trained with hidden_layer_sizes=(50,), activation=tanh, solver=adam, alpha=0.01, learning_rate=constant\n",
      "R^2 Train: 0.8077101876073303, R^2 Test: 0.7194089351533883\n",
      "\n",
      "Trying hidden_layer_sizes=(50,), activation=tanh, solver=adam, alpha=0.01, learning_rate=adaptive\n",
      "Neural network trained with hidden_layer_sizes=(50,), activation=tanh, solver=adam, alpha=0.01, learning_rate=adaptive\n",
      "R^2 Train: 0.8077101876073303, R^2 Test: 0.7194089351533883\n",
      "\n",
      "Trying hidden_layer_sizes=(50,), activation=tanh, solver=sgd, alpha=0.0001, learning_rate=constant\n",
      "Neural network trained with hidden_layer_sizes=(50,), activation=tanh, solver=sgd, alpha=0.0001, learning_rate=constant\n",
      "R^2 Train: 0.8828553368258629, R^2 Test: 0.7775720669372267\n",
      "\n",
      "Trying hidden_layer_sizes=(50,), activation=tanh, solver=sgd, alpha=0.0001, learning_rate=adaptive\n",
      "Neural network trained with hidden_layer_sizes=(50,), activation=tanh, solver=sgd, alpha=0.0001, learning_rate=adaptive\n",
      "R^2 Train: 0.8855466893958328, R^2 Test: 0.7786460406213545\n",
      "\n",
      "Trying hidden_layer_sizes=(50,), activation=tanh, solver=sgd, alpha=0.001, learning_rate=constant\n",
      "Neural network trained with hidden_layer_sizes=(50,), activation=tanh, solver=sgd, alpha=0.001, learning_rate=constant\n",
      "R^2 Train: 0.8750189155281483, R^2 Test: 0.7711157447805543\n",
      "\n",
      "Trying hidden_layer_sizes=(50,), activation=tanh, solver=sgd, alpha=0.001, learning_rate=adaptive\n",
      "Neural network trained with hidden_layer_sizes=(50,), activation=tanh, solver=sgd, alpha=0.001, learning_rate=adaptive\n",
      "R^2 Train: 0.880915045316105, R^2 Test: 0.7734717758753795\n",
      "\n",
      "Trying hidden_layer_sizes=(50,), activation=tanh, solver=sgd, alpha=0.01, learning_rate=constant\n",
      "Neural network trained with hidden_layer_sizes=(50,), activation=tanh, solver=sgd, alpha=0.01, learning_rate=constant\n",
      "R^2 Train: 0.8765237075054495, R^2 Test: 0.7709954186485853\n",
      "\n",
      "Trying hidden_layer_sizes=(50,), activation=tanh, solver=sgd, alpha=0.01, learning_rate=adaptive\n",
      "Neural network trained with hidden_layer_sizes=(50,), activation=tanh, solver=sgd, alpha=0.01, learning_rate=adaptive\n",
      "R^2 Train: 0.8799999220760824, R^2 Test: 0.7677496241115211\n",
      "\n",
      "Trying hidden_layer_sizes=(100,), activation=relu, solver=adam, alpha=0.0001, learning_rate=constant\n",
      "Neural network trained with hidden_layer_sizes=(100,), activation=relu, solver=adam, alpha=0.0001, learning_rate=constant\n",
      "R^2 Train: 0.7494489138402389, R^2 Test: 0.6799432320439998\n",
      "\n",
      "Trying hidden_layer_sizes=(100,), activation=relu, solver=adam, alpha=0.0001, learning_rate=adaptive\n",
      "Neural network trained with hidden_layer_sizes=(100,), activation=relu, solver=adam, alpha=0.0001, learning_rate=adaptive\n",
      "R^2 Train: 0.7494489138402389, R^2 Test: 0.6799432320439998\n",
      "\n",
      "Trying hidden_layer_sizes=(100,), activation=relu, solver=adam, alpha=0.001, learning_rate=constant\n",
      "Neural network trained with hidden_layer_sizes=(100,), activation=relu, solver=adam, alpha=0.001, learning_rate=constant\n",
      "R^2 Train: 0.7689970358847554, R^2 Test: 0.6934720672229323\n",
      "\n",
      "Trying hidden_layer_sizes=(100,), activation=relu, solver=adam, alpha=0.001, learning_rate=adaptive\n",
      "Neural network trained with hidden_layer_sizes=(100,), activation=relu, solver=adam, alpha=0.001, learning_rate=adaptive\n",
      "R^2 Train: 0.7689970358847554, R^2 Test: 0.6934720672229323\n",
      "\n",
      "Trying hidden_layer_sizes=(100,), activation=relu, solver=adam, alpha=0.01, learning_rate=constant\n",
      "Neural network trained with hidden_layer_sizes=(100,), activation=relu, solver=adam, alpha=0.01, learning_rate=constant\n",
      "R^2 Train: 0.7610086253753359, R^2 Test: 0.6880165511042273\n",
      "\n",
      "Trying hidden_layer_sizes=(100,), activation=relu, solver=adam, alpha=0.01, learning_rate=adaptive\n",
      "Neural network trained with hidden_layer_sizes=(100,), activation=relu, solver=adam, alpha=0.01, learning_rate=adaptive\n",
      "R^2 Train: 0.7610086253753359, R^2 Test: 0.6880165511042273\n",
      "\n",
      "Trying hidden_layer_sizes=(100,), activation=relu, solver=sgd, alpha=0.0001, learning_rate=constant\n",
      "Neural network trained with hidden_layer_sizes=(100,), activation=relu, solver=sgd, alpha=0.0001, learning_rate=constant\n",
      "R^2 Train: 0.6895107623512642, R^2 Test: 0.626716786865598\n",
      "\n",
      "Trying hidden_layer_sizes=(100,), activation=relu, solver=sgd, alpha=0.0001, learning_rate=adaptive\n",
      "Neural network trained with hidden_layer_sizes=(100,), activation=relu, solver=sgd, alpha=0.0001, learning_rate=adaptive\n",
      "R^2 Train: 0.7593876997423207, R^2 Test: 0.6708522078330124\n",
      "\n",
      "Trying hidden_layer_sizes=(100,), activation=relu, solver=sgd, alpha=0.001, learning_rate=constant\n",
      "Neural network trained with hidden_layer_sizes=(100,), activation=relu, solver=sgd, alpha=0.001, learning_rate=constant\n",
      "R^2 Train: 0.6572999078105207, R^2 Test: 0.5973220588164752\n",
      "\n",
      "Trying hidden_layer_sizes=(100,), activation=relu, solver=sgd, alpha=0.001, learning_rate=adaptive\n",
      "Neural network trained with hidden_layer_sizes=(100,), activation=relu, solver=sgd, alpha=0.001, learning_rate=adaptive\n",
      "R^2 Train: 0.771758981831415, R^2 Test: 0.672471837032924\n",
      "\n",
      "Trying hidden_layer_sizes=(100,), activation=relu, solver=sgd, alpha=0.01, learning_rate=constant\n",
      "Neural network trained with hidden_layer_sizes=(100,), activation=relu, solver=sgd, alpha=0.01, learning_rate=constant\n",
      "R^2 Train: 0.6946213420012939, R^2 Test: 0.6197170272936476\n",
      "\n",
      "Trying hidden_layer_sizes=(100,), activation=relu, solver=sgd, alpha=0.01, learning_rate=adaptive\n",
      "Neural network trained with hidden_layer_sizes=(100,), activation=relu, solver=sgd, alpha=0.01, learning_rate=adaptive\n",
      "R^2 Train: 0.7865231987276676, R^2 Test: 0.6824014805923129\n",
      "\n",
      "Trying hidden_layer_sizes=(100,), activation=tanh, solver=adam, alpha=0.0001, learning_rate=constant\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Anaconda\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (2000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural network trained with hidden_layer_sizes=(100,), activation=tanh, solver=adam, alpha=0.0001, learning_rate=constant\n",
      "R^2 Train: 0.8977471259661423, R^2 Test: 0.7689377005261147\n",
      "\n",
      "Trying hidden_layer_sizes=(100,), activation=tanh, solver=adam, alpha=0.0001, learning_rate=adaptive\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Anaconda\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (2000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural network trained with hidden_layer_sizes=(100,), activation=tanh, solver=adam, alpha=0.0001, learning_rate=adaptive\n",
      "R^2 Train: 0.8977471259661423, R^2 Test: 0.7689377005261147\n",
      "\n",
      "Trying hidden_layer_sizes=(100,), activation=tanh, solver=adam, alpha=0.001, learning_rate=constant\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Anaconda\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (2000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural network trained with hidden_layer_sizes=(100,), activation=tanh, solver=adam, alpha=0.001, learning_rate=constant\n",
      "R^2 Train: 0.8977170969160684, R^2 Test: 0.7689912013039671\n",
      "\n",
      "Trying hidden_layer_sizes=(100,), activation=tanh, solver=adam, alpha=0.001, learning_rate=adaptive\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Anaconda\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (2000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural network trained with hidden_layer_sizes=(100,), activation=tanh, solver=adam, alpha=0.001, learning_rate=adaptive\n",
      "R^2 Train: 0.8977170969160684, R^2 Test: 0.7689912013039671\n",
      "\n",
      "Trying hidden_layer_sizes=(100,), activation=tanh, solver=adam, alpha=0.01, learning_rate=constant\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Anaconda\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (2000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural network trained with hidden_layer_sizes=(100,), activation=tanh, solver=adam, alpha=0.01, learning_rate=constant\n",
      "R^2 Train: 0.8974555085790203, R^2 Test: 0.7695631764701274\n",
      "\n",
      "Trying hidden_layer_sizes=(100,), activation=tanh, solver=adam, alpha=0.01, learning_rate=adaptive\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Anaconda\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (2000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural network trained with hidden_layer_sizes=(100,), activation=tanh, solver=adam, alpha=0.01, learning_rate=adaptive\n",
      "R^2 Train: 0.8974555085790203, R^2 Test: 0.7695631764701274\n",
      "\n",
      "Trying hidden_layer_sizes=(100,), activation=tanh, solver=sgd, alpha=0.0001, learning_rate=constant\n",
      "Neural network trained with hidden_layer_sizes=(100,), activation=tanh, solver=sgd, alpha=0.0001, learning_rate=constant\n",
      "R^2 Train: 0.9310115229817559, R^2 Test: 0.7989548514882601\n",
      "\n",
      "Trying hidden_layer_sizes=(100,), activation=tanh, solver=sgd, alpha=0.0001, learning_rate=adaptive\n",
      "Neural network trained with hidden_layer_sizes=(100,), activation=tanh, solver=sgd, alpha=0.0001, learning_rate=adaptive\n",
      "R^2 Train: 0.9323066970414048, R^2 Test: 0.801104403312138\n",
      "\n",
      "Trying hidden_layer_sizes=(100,), activation=tanh, solver=sgd, alpha=0.001, learning_rate=constant\n",
      "Neural network trained with hidden_layer_sizes=(100,), activation=tanh, solver=sgd, alpha=0.001, learning_rate=constant\n",
      "R^2 Train: 0.933247199419081, R^2 Test: 0.7956468015480893\n",
      "\n",
      "Trying hidden_layer_sizes=(100,), activation=tanh, solver=sgd, alpha=0.001, learning_rate=adaptive\n",
      "Neural network trained with hidden_layer_sizes=(100,), activation=tanh, solver=sgd, alpha=0.001, learning_rate=adaptive\n",
      "R^2 Train: 0.9363890763929993, R^2 Test: 0.7999587727269063\n",
      "\n",
      "Trying hidden_layer_sizes=(100,), activation=tanh, solver=sgd, alpha=0.01, learning_rate=constant\n",
      "Neural network trained with hidden_layer_sizes=(100,), activation=tanh, solver=sgd, alpha=0.01, learning_rate=constant\n",
      "R^2 Train: 0.9334854330666771, R^2 Test: 0.8063906346174494\n",
      "\n",
      "Trying hidden_layer_sizes=(100,), activation=tanh, solver=sgd, alpha=0.01, learning_rate=adaptive\n",
      "Neural network trained with hidden_layer_sizes=(100,), activation=tanh, solver=sgd, alpha=0.01, learning_rate=adaptive\n",
      "R^2 Train: 0.936232754426899, R^2 Test: 0.8064762484090235\n",
      "\n",
      "Trying hidden_layer_sizes=(200,), activation=relu, solver=adam, alpha=0.0001, learning_rate=constant\n",
      "Neural network trained with hidden_layer_sizes=(200,), activation=relu, solver=adam, alpha=0.0001, learning_rate=constant\n",
      "R^2 Train: 0.7887098379577153, R^2 Test: 0.7075744763776701\n",
      "\n",
      "Trying hidden_layer_sizes=(200,), activation=relu, solver=adam, alpha=0.0001, learning_rate=adaptive\n",
      "Neural network trained with hidden_layer_sizes=(200,), activation=relu, solver=adam, alpha=0.0001, learning_rate=adaptive\n",
      "R^2 Train: 0.7887098379577153, R^2 Test: 0.7075744763776701\n",
      "\n",
      "Trying hidden_layer_sizes=(200,), activation=relu, solver=adam, alpha=0.001, learning_rate=constant\n",
      "Neural network trained with hidden_layer_sizes=(200,), activation=relu, solver=adam, alpha=0.001, learning_rate=constant\n",
      "R^2 Train: 0.8011216331104135, R^2 Test: 0.7161451734211907\n",
      "\n",
      "Trying hidden_layer_sizes=(200,), activation=relu, solver=adam, alpha=0.001, learning_rate=adaptive\n",
      "Neural network trained with hidden_layer_sizes=(200,), activation=relu, solver=adam, alpha=0.001, learning_rate=adaptive\n",
      "R^2 Train: 0.8011216331104135, R^2 Test: 0.7161451734211907\n",
      "\n",
      "Trying hidden_layer_sizes=(200,), activation=relu, solver=adam, alpha=0.01, learning_rate=constant\n",
      "Neural network trained with hidden_layer_sizes=(200,), activation=relu, solver=adam, alpha=0.01, learning_rate=constant\n",
      "R^2 Train: 0.8206954783437588, R^2 Test: 0.7206880576719189\n",
      "\n",
      "Trying hidden_layer_sizes=(200,), activation=relu, solver=adam, alpha=0.01, learning_rate=adaptive\n",
      "Neural network trained with hidden_layer_sizes=(200,), activation=relu, solver=adam, alpha=0.01, learning_rate=adaptive\n",
      "R^2 Train: 0.8206954783437588, R^2 Test: 0.7206880576719189\n",
      "\n",
      "Trying hidden_layer_sizes=(200,), activation=relu, solver=sgd, alpha=0.0001, learning_rate=constant\n",
      "Neural network trained with hidden_layer_sizes=(200,), activation=relu, solver=sgd, alpha=0.0001, learning_rate=constant\n",
      "R^2 Train: 0.7039764356340613, R^2 Test: 0.6464455137264197\n",
      "\n",
      "Trying hidden_layer_sizes=(200,), activation=relu, solver=sgd, alpha=0.0001, learning_rate=adaptive\n",
      "Neural network trained with hidden_layer_sizes=(200,), activation=relu, solver=sgd, alpha=0.0001, learning_rate=adaptive\n",
      "R^2 Train: 0.8379886894764614, R^2 Test: 0.7067465273810944\n",
      "\n",
      "Trying hidden_layer_sizes=(200,), activation=relu, solver=sgd, alpha=0.001, learning_rate=constant\n",
      "Neural network trained with hidden_layer_sizes=(200,), activation=relu, solver=sgd, alpha=0.001, learning_rate=constant\n",
      "R^2 Train: 0.7399605066416097, R^2 Test: 0.6765549814677474\n",
      "\n",
      "Trying hidden_layer_sizes=(200,), activation=relu, solver=sgd, alpha=0.001, learning_rate=adaptive\n",
      "Neural network trained with hidden_layer_sizes=(200,), activation=relu, solver=sgd, alpha=0.001, learning_rate=adaptive\n",
      "R^2 Train: 0.8372519804300926, R^2 Test: 0.7226589116831134\n",
      "\n",
      "Trying hidden_layer_sizes=(200,), activation=relu, solver=sgd, alpha=0.01, learning_rate=constant\n",
      "Neural network trained with hidden_layer_sizes=(200,), activation=relu, solver=sgd, alpha=0.01, learning_rate=constant\n",
      "R^2 Train: 0.7396654843643047, R^2 Test: 0.6690502833894256\n",
      "\n",
      "Trying hidden_layer_sizes=(200,), activation=relu, solver=sgd, alpha=0.01, learning_rate=adaptive\n",
      "Neural network trained with hidden_layer_sizes=(200,), activation=relu, solver=sgd, alpha=0.01, learning_rate=adaptive\n",
      "R^2 Train: 0.8407397705780064, R^2 Test: 0.7067094387627361\n",
      "\n",
      "Trying hidden_layer_sizes=(200,), activation=tanh, solver=adam, alpha=0.0001, learning_rate=constant\n",
      "Neural network trained with hidden_layer_sizes=(200,), activation=tanh, solver=adam, alpha=0.0001, learning_rate=constant\n",
      "R^2 Train: 0.911383348124434, R^2 Test: 0.7668571317825862\n",
      "\n",
      "Trying hidden_layer_sizes=(200,), activation=tanh, solver=adam, alpha=0.0001, learning_rate=adaptive\n",
      "Neural network trained with hidden_layer_sizes=(200,), activation=tanh, solver=adam, alpha=0.0001, learning_rate=adaptive\n",
      "R^2 Train: 0.911383348124434, R^2 Test: 0.7668571317825862\n",
      "\n",
      "Trying hidden_layer_sizes=(200,), activation=tanh, solver=adam, alpha=0.001, learning_rate=constant\n",
      "Neural network trained with hidden_layer_sizes=(200,), activation=tanh, solver=adam, alpha=0.001, learning_rate=constant\n",
      "R^2 Train: 0.9114468766641264, R^2 Test: 0.7664520402500468\n",
      "\n",
      "Trying hidden_layer_sizes=(200,), activation=tanh, solver=adam, alpha=0.001, learning_rate=adaptive\n",
      "Neural network trained with hidden_layer_sizes=(200,), activation=tanh, solver=adam, alpha=0.001, learning_rate=adaptive\n",
      "R^2 Train: 0.9114468766641264, R^2 Test: 0.7664520402500468\n",
      "\n",
      "Trying hidden_layer_sizes=(200,), activation=tanh, solver=adam, alpha=0.01, learning_rate=constant\n",
      "Neural network trained with hidden_layer_sizes=(200,), activation=tanh, solver=adam, alpha=0.01, learning_rate=constant\n",
      "R^2 Train: 0.9123018662153479, R^2 Test: 0.7667244666160109\n",
      "\n",
      "Trying hidden_layer_sizes=(200,), activation=tanh, solver=adam, alpha=0.01, learning_rate=adaptive\n",
      "Neural network trained with hidden_layer_sizes=(200,), activation=tanh, solver=adam, alpha=0.01, learning_rate=adaptive\n",
      "R^2 Train: 0.9123018662153479, R^2 Test: 0.7667244666160109\n",
      "\n",
      "Trying hidden_layer_sizes=(200,), activation=tanh, solver=sgd, alpha=0.0001, learning_rate=constant\n",
      "Neural network trained with hidden_layer_sizes=(200,), activation=tanh, solver=sgd, alpha=0.0001, learning_rate=constant\n",
      "R^2 Train: 0.9600865780439204, R^2 Test: 0.8309723358830572\n",
      "\n",
      "Trying hidden_layer_sizes=(200,), activation=tanh, solver=sgd, alpha=0.0001, learning_rate=adaptive\n",
      "Neural network trained with hidden_layer_sizes=(200,), activation=tanh, solver=sgd, alpha=0.0001, learning_rate=adaptive\n",
      "R^2 Train: 0.9616416060607201, R^2 Test: 0.8320894015545486\n",
      "\n",
      "Trying hidden_layer_sizes=(200,), activation=tanh, solver=sgd, alpha=0.001, learning_rate=constant\n",
      "Neural network trained with hidden_layer_sizes=(200,), activation=tanh, solver=sgd, alpha=0.001, learning_rate=constant\n",
      "R^2 Train: 0.9600756226689489, R^2 Test: 0.8309761449501758\n",
      "\n",
      "Trying hidden_layer_sizes=(200,), activation=tanh, solver=sgd, alpha=0.001, learning_rate=adaptive\n",
      "Neural network trained with hidden_layer_sizes=(200,), activation=tanh, solver=sgd, alpha=0.001, learning_rate=adaptive\n",
      "R^2 Train: 0.9616382869120041, R^2 Test: 0.8320378509683979\n",
      "\n",
      "Trying hidden_layer_sizes=(200,), activation=tanh, solver=sgd, alpha=0.01, learning_rate=constant\n",
      "Neural network trained with hidden_layer_sizes=(200,), activation=tanh, solver=sgd, alpha=0.01, learning_rate=constant\n",
      "R^2 Train: 0.960147440688726, R^2 Test: 0.8312753499955262\n",
      "\n",
      "Trying hidden_layer_sizes=(200,), activation=tanh, solver=sgd, alpha=0.01, learning_rate=adaptive\n",
      "Neural network trained with hidden_layer_sizes=(200,), activation=tanh, solver=sgd, alpha=0.01, learning_rate=adaptive\n",
      "R^2 Train: 0.9616956789242057, R^2 Test: 0.8316566114473007\n",
      "Best parameters: {'hidden_layer_sizes': (200,), 'activation': 'tanh', 'solver': 'sgd', 'alpha': 0.0001, 'learning_rate': 'adaptive'}\n",
      "Best R^2 Test: 0.8320894015545486\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameter values to try\n",
    "hidden_layer_sizes = [(50,), (100,), (200,)]\n",
    "activations = ['relu', 'tanh']\n",
    "solvers = ['adam', 'sgd']\n",
    "alphas = [0.0001, 0.001, 0.01]\n",
    "learning_rates = ['constant', 'adaptive']\n",
    "\n",
    "best_r2_test = -np.inf\n",
    "best_params = {}\n",
    "#refered to chatgpt for simplified code structure\n",
    "# Perform trial and error\n",
    "for hls in hidden_layer_sizes:\n",
    "    for act in activations:\n",
    "        for sol in solvers:\n",
    "            for alpha in alphas:\n",
    "                for lr in learning_rates:\n",
    "                    print(f\"\\nTrying hidden_layer_sizes={hls}, activation={act}, solver={sol}, alpha={alpha}, learning_rate={lr}\")\n",
    "                    r2_train, r2_test = train_evaluate_nn(X_train_scaled, y_train, X_test_scaled, y_test, hidden_layer_sizes=hls, activation=act, solver=sol, alpha=alpha, learning_rate=lr)\n",
    "                    if r2_test > best_r2_test:\n",
    "                        best_r2_test = r2_test\n",
    "                        best_params = {'hidden_layer_sizes': hls, 'activation': act, 'solver': sol, 'alpha': alpha, 'learning_rate': lr}\n",
    "\n",
    "print(f\"Best parameters: {best_params}\")\n",
    "print(f\"Best R^2 Test: {best_r2_test}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural network trained with early stopping and validation fraction=0.1\n",
      "R^2 Train: 0.8053613161778965, R^2 Test: 0.7520639231514127\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# Best parameters from previous hyperparameter tuning\n",
    "best_params = {\n",
    "    'hidden_layer_sizes': (200,),\n",
    "    'activation': 'tanh',\n",
    "    'solver': 'sgd',\n",
    "    'alpha': 0.0001,\n",
    "    'learning_rate': 'adaptive'\n",
    "}\n",
    "\n",
    "def train_evaluate_nn_early_stopping(X_train, y_train, X_test, y_test, best_params, validation_fraction=0.1):\n",
    "    model = MLPRegressor(\n",
    "        hidden_layer_sizes=best_params['hidden_layer_sizes'],\n",
    "        activation=best_params['activation'],\n",
    "        solver=best_params['solver'],\n",
    "        alpha=best_params['alpha'],\n",
    "        learning_rate=best_params['learning_rate'],\n",
    "        max_iter=2000,\n",
    "        early_stopping=True,\n",
    "        validation_fraction=validation_fraction,\n",
    "        random_state=42\n",
    "    )\n",
    "    model.fit(X_train, y_train)\n",
    "    print(f\"Neural network trained with early stopping and validation fraction={validation_fraction}\")\n",
    "\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    y_test_pred = model.predict(X_test)\n",
    "    \n",
    "    r2_train = r2_score(y_train, y_train_pred)\n",
    "    r2_test = r2_score(y_test, y_test_pred)\n",
    "    \n",
    "    print(f\"R^2 Train: {r2_train}, R^2 Test: {r2_test}\")\n",
    "    \n",
    "    return model, r2_train, r2_test\n",
    "\n",
    "# Train and evaluate the model with early stopping\n",
    "final_model_early_stopping, r2_train_early_stopping, r2_test_early_stopping = train_evaluate_nn_early_stopping(X_train_scaled, y_train, X_test_scaled, y_test, best_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
